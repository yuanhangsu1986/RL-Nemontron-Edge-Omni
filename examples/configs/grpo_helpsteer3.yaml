# TODO @rayen: remove this file after refactor
defaults: grpo_math_1B.yaml
grpo:
  max_num_steps: 500
checkpointing:
  checkpoint_dir: results/grpo-helpsteer3
  metric_name: val_reward
policy:
  model_name: meta-llama/Llama-3.2-1B-Instruct
  max_total_sequence_length: 2048
  dynamic_batching:
    enabled: true
  sequence_packing:
    enabled: false
data:
  prompt_file: null
  dataset_name: HelpSteer3
env:
  helpsteer3:
    num_workers: 8
    reward_model: preference_based
logger:
  wandb_enabled: true
  monitor_gpus: false
  wandb:
    project: grpo-helpsteer3
    name: grpo-helpsteer3
  tensorboard:
    log_dir: tb_logs-grpo-helpsteer3
  mlflow:
    experiment_name: grpo-helpsteer3
    run_name: grpo-helpsteer3
cluster:
  gpus_per_node: 8
