[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["nemo_rl"]

[tool.setuptools.dynamic]
version = { attr = "nemo_rl.__version__" }                      # any module attribute compatible with ast.literal_eval
readme = { file = "README.md", content-type = "text/markdown" }

[project]
name = "nemo-rl"
dynamic = ["version", "readme"]
description = "NeMo RL: A Scalable and Efficient Post-Training Library for Models Ranging from 1 GPU to 1000s, and from Tiny to >100B Parameters"
requires-python = ">=3.12"
license = { text = "Apache 2.0" }
dependencies = [
  "setuptools",
  "ninja",                                                                                                            # for flash-attn parallel build
  "torch==2.8.0",
  "triton; sys_platform == 'linux' and (platform_machine == 'x86_64' or platform_machine == 'aarch64')",
  "colored==2.2.3",
  "ray[default]==2.49.2",
  "transformers>=4.55.4",
  "wandb",
  "numpy",
  "datasets>=4.0.0",
  "rich",
  "math-verify",
  "accelerate>=0.26",
  "tensorboard",
  "omegaconf",
  "torchdata",
  "nvidia-ml-py",
  "hydra-core",
  "tiktoken",
  "blobfile",
  "debugpy",
  "nvtx",
  "matplotlib",
  "plotly",
  "sympy>=1.14.0",
  "pillow>=11.3.0",
  "torchvision>=0.22.0",
  "num2words>=0.5.14",                                                                                                # for SmolVLM
  "mlflow>=3.5.0,<3.6.0",
  "nvidia-nvshmem-cu12; sys_platform == 'linux' and (platform_machine == 'x86_64' or platform_machine == 'aarch64')", # for deep_ep build
  "swanlab",
  "pyzmq",
]

[project.optional-dependencies]
# Currently unused, but after https://github.com/NVIDIA-NeMo/RL/issues/501 is resolved, we should use this for the "BASE" PYEXECUTABLE
automodel = [
  "nemo-automodel",
  # Flash-attn version should be selected to satisfy both TE + vLLM requirements (xformers in particular)
  # https://github.com/NVIDIA/TransformerEngine/blob/v2.3/transformer_engine/pytorch/attention/dot_product_attention/utils.py#L108
  # https://github.com/facebookresearch/xformers/blob/8354497deb2c04c67fbb2e2ad911e86530da0e90/xformers/ops/fmha/flash.py#L76
  "vllm==0.11.0",      # Remove this once https://github.com/NVIDIA-NeMo/RL/issues/811 resolved
  "flash-attn==2.8.1",
  "mamba-ssm",
  "causal-conv1d",
]
vllm = [
  "cuda-python",
  "deep_gemm @ git+https://github.com/deepseek-ai/DeepGEMM.git@7b6b5563b9d4c1ae07ffbce7f78ad3ac9204827c",
  # deep_ep also needs libibverbs-dev
  # sudo apt-get update
  # sudo apt-get install libibverbs-dev
  "deep_ep @ git+https://github.com/deepseek-ai/DeepEP.git@e3908bf5bd0cc6265bcb225d15cd8c996d4759ef",
  "vllm==0.11.0",
  "num2words>=0.5.14",
  # Remove this once https://github.com/NVIDIA-NeMo/RL/issues/501 resolved
  "flash-attn==2.8.1",
  # Remove this once https://github.com/NVIDIA-NeMo/RL/issues/501 resolved
  "mamba-ssm",
  # Remove this once https://github.com/NVIDIA-NeMo/RL/issues/501 resolved
  "causal-conv1d",
]
mcore = [
  # also need cudnn (https://developer.nvidia.com/cudnn-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_network)
  # wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb
  # sudo dpkg -i cuda-keyring_1.1-1_all.deb
  # sudo apt-get update
  # sudo apt-get install cudnn-cuda-12

  # This dependency also needs to be compatible with the spec in Megatron-Bridge/pyproject.toml.
  # It is specified here since we don't directly use Megatron-Bridge/pyproject.toml, but a proxy setup.py+pyproject.toml combo
  # outside to allow "optionally" installing the megatron path. It's simpler to deal with transformer-engine here in the NeMo RL pyproject.toml
  "transformer-engine[pytorch]==2.8.0",
  "megatron-core",
  "megatron-bridge",
  # Remove this once https://github.com/NVIDIA-NeMo/RL/issues/501 resolved
  "vllm==0.11.0",
  # Flash-attn version should be selected to satisfy both TE + vLLM requirements (xformers in particular)
  # https://github.com/NVIDIA/TransformerEngine/blob/v2.3/transformer_engine/pytorch/attention/dot_product_attention/utils.py#L108
  # https://github.com/facebookresearch/xformers/blob/8354497deb2c04c67fbb2e2ad911e86530da0e90/xformers/ops/fmha/flash.py#L76
  "flash-attn==2.8.1",
]
penguin = ["penguin"]

[dependency-groups]

# This is a default group so that we install these even with bare `uv sync`
build = [
  # Build requirement for TE
  "torch==2.8.0",
  # Build requirement for TE
  "setuptools",
  "packaging",
  "einops",
  # Build requirement for nemo_run
  "hatchling",
  # Build requirement for mcore
  "pybind11",
  # Build requirement for flash-attn
  "psutil",
]
docs = [
  "sphinx",
  "sphinx-autobuild",          # For live doc serving while editing docs
  "sphinx-autodoc2",           # For documenting Python API
  "sphinx-copybutton",         # Adds a copy button for code blocks
  "sphinx-design",             # For design components in docs
  "myst_parser",               # For our markdown docs
  "nvidia-sphinx-theme",       # Our NVIDIA theme
  "gitpython>=3.1.45",         # To git-related information
  "python-dotenv",             # For environment variable management
  "sphinxcontrib-mermaid",     # For Mermaid diagram support
  "swagger-plugin-for-sphinx", # For Swagger/OpenAPI documentation
]
dev = [
  "pre-commit>=4.2.0",
  "ruff==0.9.9",
  "types-PyYAML",
  "types-requests",
  "pyrefly==0.24.2",
]
test = [
  "pytest>=7.0.0",
  "pytest-timeout",
  "pytest-cov",
  "pytest-asyncio",
  "pytest-testmon",
]

[tool.uv.sources]
megatron-core = { workspace = true }
nemo-automodel = { workspace = true }
megatron-bridge = { workspace = true }
penguin = { workspace = true }
nemo_run = { git = "https://github.com/NVIDIA-NeMo/Run", rev = "414f0077c648fde2c71bb1186e97ccbf96d6844c" }
# torch/torchvision/triton all come from the torch index in order to pick up aarch64 wheels
torch = [
  { index = "pytorch-cu129", marker = "sys_platform != 'darwin'" },
  { index = "pypi", marker = "sys_platform == 'darwin'" },
]
torchvision = [
  { index = "pytorch-cu129", marker = "sys_platform != 'darwin'" },
  { index = "pypi", marker = "sys_platform == 'darwin'" },
]
triton = [
  { index = "pytorch-cu129", marker = "sys_platform != 'darwin'" },
  { index = "pypi", marker = "sys_platform == 'darwin'" },
]
causal-conv1d = { git = "https://github.com/Dao-AILab/causal-conv1d", tag = "v1.5.0.post8" }
mamba-ssm = { git = "https://github.com/state-spaces/mamba.git", rev = "2e16fc3062cdcd4ebef27a9aa4442676e1c7edf4" }

[tool.uv.workspace]
members = [
  "3rdparty/Megatron-LM-workspace",
  "3rdparty/Automodel-workspace/Automodel",
  "3rdparty/Megatron-Bridge-workspace",
  "3rdparty/Penguin-workspace",
  # Research projects are also added here in order for them to share the global root level uv.lock.
  # If we don't do this, the research projects do not see the global uv.lock, and may mistakenly 
  # install numpy>=2.0 because nemo-rl's core [dependencies] do not pin numpy, but when you inspect
  # nemo-rl's uv.lock you'll see it's 1.X b/c megatron mandates 1.X in the optional dependencies, so
  # globally we must choose 1.X otherwise we run into pickle issues from ray.
  "research/template_project",
]

[[tool.uv.index]]
name = "pypi"
url = "https://pypi.org/simple"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu129"
url = "https://download.pytorch.org/whl/cu129"
explicit = true

[tool.uv]
preview = true # Enable preview features like extra-build-dependencies
no-build-isolation-package = [
  "transformer-engine-torch",
  "transformer-engine",
  "flash-attn",
  "mamba-ssm",
  "causal-conv1d",
  "deep_gemm",
  "deep_ep",
]
# Always apply the build group since dependencies like TE/mcore/nemo-run require build dependencies
# and this lets us assume they are implicitly installed with a simply `uv sync`. Ideally, we'd
# avoid including these in the default dependency set, but for now it's required.
default-groups = ["dev", "build"]
# Users may use different link-modes depending on their scenario:
#  --link-mode=hardlink (default on linux; may get warnings about switching to --link-mode copy if uv cache and venv on different file-systems)
#  --link-mode=copy (slower but more reliable; supresses warning)
#  --link-mode=symlink (fastest option when uv cache and venv on different file-system; caveat: venv is brittle since it depends on the environment/container)
link-mode = "copy"
# The TE override is needed because automodel/mbridge we are on is still on 2.5.0
# The opencv-python-headless override is needed because automodel pins it to 4.10.0.84, whereas vllm>=0.11.0 needs >= 4.11.0
# The transformers override is needed since automodel is still on <=4.55.4
# The timm override is needed because current automodel pins to 1.0.16. This can be removed once we move ToT automodel
override-dependencies = [
  "transformer-engine[pytorch]==2.8.0",
  "opencv-python-headless>=4.11.0",
  "transformers>=4.57.1",
  "timm<=1.0.22",
]

# Augment build dependencies for packages that need torch at build time
[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]
# Git-sourced packages CAN use match-runtime = true if we provide dependency-metadata
deep_ep = [{ requirement = "torch", match-runtime = true }]
deep_gemm = [{ requirement = "torch", match-runtime = true }]
transformer-engine = [{ requirement = "torch", match-runtime = true }]
transformer-engine-torch = [{ requirement = "torch", match-runtime = true }]
mamba-ssm = [{ requirement = "torch", match-runtime = true }]
causal-conv1d = [{ requirement = "torch", match-runtime = true }]

# Needed when building from source
[[tool.uv.dependency-metadata]]
name = "flash-attn"
requires-dist = ["torch", "einops", "setuptools", "psutil", "ninja"]

[[tool.uv.dependency-metadata]]
name = "causal-conv1d"
# This version has to match the version in the commit/rev/tag used
version = "1.5.0.post8"
requires-dist = ["torch", "packaging", "ninja"]

[[tool.uv.dependency-metadata]]
name = "mamba-ssm"
# This version has to match the version in the commit/rev/tag used
version = "2.2.4"
requires-dist = ["torch", "packaging", "ninja", "causal-conv1d"]

[[tool.uv.dependency-metadata]]
name = "deep_ep"
# This version has to match the version in the commit/rev/tag used
version = "v1.1.0+e3908bf"
requires-dist = ["torch", "packaging", "ninja"]

[[tool.uv.dependency-metadata]]
name = "deep_gemm"
# This version has to match the version in the commit/rev/tag used
version = "v2.0.0+7b6b556"
requires-dist = ["torch", "packaging", "ninja"]

[tool.black]
line-length = 120
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.venv
  | build
)/
'''

[tool.pytest.ini_options]
addopts = "--durations=15 -s -rA -x"
testpaths = ["tests"]
python_files = "test_*.py"
markers = [
  "run_first: marks tests that should run before others",
  "mcore: marks tests that require the mcore extra",
  "hf_gated: marks tests that require HuggingFace token access for gated models",
  "automodel: marks tests that require the automodel extra",
  "vllm: marks tests that require the vllm extra",
]

[tool.pyrefly]
project-includes = ["**/*"]
project-excludes = ["**/*venv/**/*"]

[tool.coverage.run]
concurrency = ["thread", "multiprocessing"]
omit = ["/tmp/*"]

[tool.coverage.paths]
source = ["nemo_rl/", "/opt/nemo-rl/nemo_rl/"]

[tool.ruff.lint]
# Enable all `pydocstyle` rules, limiting to those that adhere to the
# Google convention via `convention = "google"`, below.
select = ["D", "F"]

# - On top of the Google convention, disable `D417`, which requires
#   documentation for every function parameter.
# - F841: local variable assigned but never used (exluced to favor readability)
# TODO: Remove D10 once we are about to release to get all the docstrings written
ignore = ["D417", "D10", "F841"]

[tool.ruff.lint.pydocstyle]
convention = "google"

# Section to exclude errors for different file types
[tool.ruff.lint.per-file-ignores]
# Ignore all directories named `tests`.
"tests/**" = ["D"]
# Ignore all files that end in `_test.py`.
"*_test.py" = ["D"]
# Ignore F401 (import but unused) in __init__.py
"__init__.py" = ["F401"]
